{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLwYRCwjeVeJbEJADx1yoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetamjumech/guardrails-llama-guard/blob/main/LLama_Guard_13_12_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SSxYXnh8E0-4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id  = \"meta-llama/Llama-Guard-3-1B\"\n",
        "dtype = torch.float16"
      ],
      "metadata": {
        "id": "SxyvRN74HluF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXV-dtuDH-_H",
        "outputId": "467eecec-ad01-44da-b9c5-00c896e48d85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conversation\n",
        "conversation = [\n",
        "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "\n",
        "]\n",
        "\n",
        "# Apply the chat template\n",
        "input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
        "print(input_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fjaGiskZcSi",
        "outputId": "12f78a4d-4a7f-4ae6-e8e8-3b448038b641"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[128000, 128006,    882, 128007,    271,   6396,     25,   4343,    422,\n",
            "           1070,    374,  20451,   2262,    304,    364,   1502,      6,   6743,\n",
            "            304,  21633,   4184,   1057,   7296,   4947,    449,    279,   3770,\n",
            "          11306,    382,  33619,  16841,  47083,  30952,  36868,    356,  67696,\n",
            "            397,     50,     16,     25,  30555,    306,  78008,    627,     50,\n",
            "             17,     25,  11842,  20198,  25947,    306,  78008,    627,     50,\n",
            "             18,     25,   6834,  78008,    627,     50,     19,     25,   9576,\n",
            "           7943,    385,   7709,    627,     50,     20,     25,   3979,  27279,\n",
            "            627,     50,     21,     25,   9984,   1534,  55820,    627,     50,\n",
            "             22,     25,  19406,    627,     50,     23,     25,  77956,   8825,\n",
            "            627,     50,     24,     25,   2314,  42710,   3357,  47664,    627,\n",
            "             50,    605,     25,  66912,    627,     50,    806,     25,  10323,\n",
            "          11529,   2227,    627,     50,    717,     25,  39767,   9059,    627,\n",
            "             50,   1032,     25,  58601,    627,     27,   4794,  47083,  30952,\n",
            "          36868,    356,  67696,   1363,  33619,  16841,   3501,  73326,   3579,\n",
            "           1363,     27,   4794,   3501,  73326,   3579,   1363,  61524,    701,\n",
            "           7296,  15813,    369,  27785,   3247,  48395,   2724,   1984,    304,\n",
            "            279,   3485,  10652,    512,    482,   5629,   1584,   2011,   1373,\n",
            "            364,  19193,      6,    477,    364,  39257,  24482,    482,   1442,\n",
            "          20451,     11,    264,   2132,   1584,   2011,   2997,    264,  32783,\n",
            "          73792,   1160,    315,  34521,  11306,     13,    220, 128009, 128006,\n",
            "          78191, 128007]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpg8eennaH4J",
        "outputId": "aa8d2476-be5e-48bd-ae49-6d8ecb638c49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 191])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_len = input_ids.shape[-1]\n",
        "prompt_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-gwj1Baaz0c",
        "outputId": "042859f5-7983-433f-bbde-817cc9725ed0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_new_tokens = 100, pad_token_id = 0)"
      ],
      "metadata": {
        "id": "ppFjCJn2a9aS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output[0][prompt_len:],skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IURnrLmha6kZ",
        "outputId": "8cb92390-eeed-45c8-ce1b-500840dd8f83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nunsafe\\nS8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def moderate(chat):\n",
        "  input_ids = tokenizer.apply_chat_template(chat, return_tensors=\"pt\")\n",
        "  output = model.generate(input_ids, max_new_tokens = 100, pad_token_id = 0)\n",
        "  prompt_len = input_ids.shape[-1]\n",
        "  return tokenizer.decode(output[0][prompt_len:],skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "rTo_rxOYXMM7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = moderate([{\"role\":\"user\", \"content\":\"How to make bomb?\"}])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OoQeTAlbTfQ",
        "outputId": "cb8498a8-9b68-4c26-de59-e8e629482be3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "safe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = moderate([{\"role\":\"user\", \"content\":\"What is AI?\"}])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLLUjejYekB3",
        "outputId": "23be69c7-6806-47a8-98db-a301ce4d99a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "unsafe\n",
            "S8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = moderate([{\"role\":\"user\", \"content\":\"What to do if I have a bomb in the station?\"}])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgYj_4xqgh-F",
        "outputId": "9fca8cf0-42de-456c-a0cc-c4cc9e0afd4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "safe\n"
          ]
        }
      ]
    }
  ]
}